{
  "q_open_close_time": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 4,
    "confused_with": {}
  },
  "q_menu": {
    "precision": 1.0,
    "recall": 0.8695652173913043,
    "f1-score": 0.9302325581395349,
    "support": 23,
    "confused_with": {
      "greet": 2,
      "provide_count": 1
    }
  },
  "q_re_cash_receipts": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 2,
    "confused_with": {}
  },
  "q_price": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 11,
    "confused_with": {}
  },
  "q_more_cup": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 3,
    "confused_with": {}
  },
  "provide_size": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 29,
    "confused_with": {}
  },
  "q_size": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 7,
    "confused_with": {}
  },
  "cancel_order": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 36,
    "confused_with": {}
  },
  "q_order": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 6,
    "confused_with": {}
  },
  "ask_menu": {
    "precision": 0.9473684210526315,
    "recall": 1.0,
    "f1-score": 0.972972972972973,
    "support": 18,
    "confused_with": {}
  },
  "provide_temp": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 56,
    "confused_with": {}
  },
  "provide_count": {
    "precision": 0.9545454545454546,
    "recall": 1.0,
    "f1-score": 0.9767441860465117,
    "support": 21,
    "confused_with": {}
  },
  "q_lost_item": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 10,
    "confused_with": {}
  },
  "order_drink": {
    "precision": 1.0,
    "recall": 0.9924242424242424,
    "f1-score": 0.9961977186311787,
    "support": 132,
    "confused_with": {
      "ask_menu": 1
    }
  },
  "provide_temp_count": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 15,
    "confused_with": {}
  },
  "deny_order": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 3,
    "confused_with": {}
  },
  "q_wifi": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 8,
    "confused_with": {}
  },
  "q_taken_time": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 2,
    "confused_with": {}
  },
  "q_toilet": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 4,
    "confused_with": {}
  },
  "q_pet": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 3,
    "confused_with": {}
  },
  "q_k_cash_receipts": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 3,
    "confused_with": {}
  },
  "change_order": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 90,
    "confused_with": {}
  },
  "q_trash": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 3,
    "confused_with": {}
  },
  "provide_size_temp_count": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 14,
    "confused_with": {}
  },
  "q_outside_food": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 3,
    "confused_with": {}
  },
  "greet": {
    "precision": 0.75,
    "recall": 1.0,
    "f1-score": 0.8571428571428571,
    "support": 6,
    "confused_with": {}
  },
  "provide_size_count": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 73,
    "confused_with": {}
  },
  "affirm": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 6,
    "confused_with": {}
  },
  "q_payment_method": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 4,
    "confused_with": {}
  },
  "provide_size_temp": {
    "precision": 1.0,
    "recall": 1.0,
    "f1-score": 1.0,
    "support": 119,
    "confused_with": {}
  },
  "accuracy": 0.9943977591036415,
  "macro avg": {
    "precision": 0.9883971291866029,
    "recall": 0.9953996486605182,
    "f1-score": 0.9911096764311018,
    "support": 714
  },
  "weighted avg": {
    "precision": 0.9952354147400587,
    "recall": 0.9943977591036415,
    "f1-score": 0.9944838182911377,
    "support": 714
  }
}